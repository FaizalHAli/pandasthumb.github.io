---
layout: article
title: NFL Theorem and uniform distribution functions
date: '2006-05-29 13:45:34 -0700'
mt_id: 2230
blog_id: 2
post_id: 2230
basename: nfl_theorem_and
published: false
---
<quote author="Erik">
Ironically, even if we grant that the prior over the set of all cost functions is uniform, the NFL theorem does not say that optimization is very difficult. It actually says that, when the prior is uniform, optimization is child's play! I mean that almost literally. Almost any strategy no matter how elaborate or crude will do. If the prior over the set of cost functions is uniform, then so is the prior over the set of cost values. That means that if we sample a point in the search space we are equally likely to get a low cost value as a high cost value. Suppose that there are Y possible cost values. Then the probability a sampled point will have one of the L lowest cost values is just

r = L / Y,

regardless of which strategy that was used to decide which point to sample. The probability s that at least one of N different sampled points will have a cost value among the L best is given by

s = 1 - (1 - r)^N,

again independently of the strategy used. Is that good or bad performance? The number of points required to achieve a given performance and confidence level is

N = ln(1 - s) / ln(1 - r) ~ - ln(1 - s) / r.

After sampling 298 points the probability that at least one of them is among the best 1% is 0.95. After 916 sampled points the same probability is 0.9999. If instead we want a point among the best 0.1% we need to sample 2994 points to find one with probability 0.95, or approximately 9206 points to find one with probability 0.9999. That kind of performance may not be satisfactory when the optimization must be done very fast in real-time under critical conditions, but it is good for most purposes. Certainly our universe would seem to be able to spare the time necessary to sample 9206 points. This is why Thomas English wrote
<quote author="Tom English">"The maligned uniform distribution is actually benign. The probability of finding one of the better points with n evaluations does not depend on the size of the domain [7]. For instance, 916 evaluations uncover with 99.99% certainty a point that is better than 99% of the domain. What is remarkable about NFL and the uniform is not just that simple enumeration of points is optimal, but that it is highly effective." 
</quote>

<quote author="Erik">
It may interest you to know that a rigorous information-theoretic analysis of black-box optimization has been done. It is reported in the following article (which is also the source of the quote above):

English T. (1999) "Some Information Theoretic Results On Evolutionary Optimization", Proceedings of the 1999 Congress on Evolutionary Computation: CEC99, pp. 788-795
</quote>
</quote>

<url href="http://www.iscid.org/boards/ubb-get_topic-f-6-t-000081.html">ISCID discussion</url>


<!--more-->

<quote author="Erik">In addition to the ironic assumption of a uniform prior, there is also the problem that Dembski has misunderstood the role of a cost/objective/fitness function in optimization. As I have <url href="http://www.iscid.org/ubbcgi/ultimatebb.cgi?ubb=get_topic&f=6&t=000009&p=2">pointed out in another thread</url>, the cost function is the mathematical representation of the problem to be solved. It is part our definition of the mathematical problem, not, as Dembski seems to think, a part the attempt to solve it. Such conceptual mistakes deprive Dembski's argument of all its force.</quote>

