---
layout: article
title: A quick explanation of Wasserstein Metric
date: '2004-08-12 18:26:33 -0700'
author: PvM
mt_id: 381
blog_id: 2
post_id: 381
basename: a_quick_explana
---
<b>Note that Dembski has uploaded a <a href="http://www.designinference.com/documents/2004.08.Variational_Information.pdf">revised manuscript</a> which now correctly attributes the measure to Renyi and thanks the many critics for their contributions</b>

I am not a mathematician but let me give it a try and others can amend and revise my comments.

The Kantorovich/Wasserstein distance metric is also known under such names as the Dudley, Fortet Mourier, Mallows and is defined as follows.

<eqn>d_p(F,G) = \overset{\inf}{\tau_{x,y}} \lbrace E |x-y|^{\frac{1}{p}} \rbrace</eqn>

where <eqn>E(x)</eqn> refers to the expectation of the random variable x and <eqn>\inf</eqn> means that the minimum is sought on all random variables X which take a distribution F and random variables Y which take a distribution G.

where <eqn>\tau_{x,y}</eqn> is the set of all joint distributions of random variables X and Y whose marginal distributions are F and G. 



<!--more-->

These metrics define a 'distance' between two stochastic distributions and are one of many such metrics that have been mathematically defined. There is a good paper on many of these metrics <a href="http://www.math.hmc.edu/~su/papers.dir/metrics.pdf">On Choosing and Bounding Probability Metrics</a>.  Different circumstances ask for different distance metrics.

These metrics have found applicability in non-linear equations,  variational approaches to entropy dissipation, <a href="http://www.pnas.org/cgi/content/full/100/12/6922">Phase transitions and symmetry breaking in singular diffusion</a>, random walks, Markov processes and many more. Needless to say these metrics are quite commonly applied in a variety of applications. Applications of this metric to Markov processes may be of interest to evolutionary theory.


Adapted from <a href="http://www.statslab.cam.ac.uk/Reports/2004/2004-04.pdf">Central Limit Theorem and convergence to stable laws in Mallows distance</a>

Another way of looking at this is by assuming one has two samples X and Y of the same size <eqn>X=\lbrace x_1,...,x_n \rbrace</eqn> and <eqn>Y=\lbrace y_1,...,y_n \rbrace</eqn>. The Mallows distance between empirical distributions is

<eqn>d_p(X,Y)= ( \frac{1}{n} \overset{min}{(j_1,...,j_n)} \sum_{i=1}^{n} \lvert x_i - y_i \rvert   )^\frac{1}{p}</eqn>

where the minimum is taken over all possible permutations of <eqn>\lbrace 1, ..., n \rbrace</eqn>

Rachev, S. T. (1984), The Monge-Kantorovich problem on mass transfer and
its applications in stochastics, Theor. Probab. Appl., 29, 647-676. 

As far as some interesting applications are concerned

<a href="http://www.math.missouri.edu/~calvin/entropyfiles/ABC.pdf">Minimal Entropy Probability Paths Between Genome Families</a> 
