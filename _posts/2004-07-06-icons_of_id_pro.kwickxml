---
layout: article
title: 'Icons of ID: Probability as information'
date: '2004-07-06 08:50:04 -0700'
mt_id: 284
blog_id: 2
post_id: 284
basename: icons_of_id_pro
---
In this episode of <i>Icons of ID</i> I will take a quick look at how the definition of information used by ID proponents is nothing more than an argument from probability. In fact when ID proponents claim that chance and regularity cannot create complex specified information (CSI) all they are saying is that such pathways, as far as we know, are improbable. If a pathway is found that is probable, the measure of information, which is confusingly linked to probability decreases.
In fact, I argue, that intelligent designers similarly cannot generate specified complex information since the probability of intelligent designers designing is close to 1.



<!--more-->

<h>Information and probability</h>

Elsberry and Willkins on CSI

<quote>
Then again, the choice of the term "complex specified information" is itself extremely problematic, since for Dembski "complex" means neither \complicated" as in ordinary speech, nor "high Kolmogorov complexity" as understood by algorithmic information theorists. Instead, Dembski uses \complex" as a synonym for "improbable".
</quote>

So how does Dembski define information?

<eqn>I(X)= - \log_2 P(X)</eqn>

So in other words, information is the log of the probability. But what probability is this? Others have shown how Dembski is unclear on this issue and often moves between uniform probabilities or actual probabilities, whenever it seems better to do so. 
Dembski mentions in NFL that this measure of information is similar to Shannon information. In fact Shannon's <b>entropy</b> is the average of Dembski's information measure. This confusion about information and entropy is not limited to Dembski's writings however so let's look at Shannon entropy and information in more detail.

<h>Claude: Shannon: A mathematical theory of communication</h>

In 1948 Shannon published one of his seminal papers on <a href="http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">A mathematical theory of communication</a>.

Shannon shows that the logarithm is the natural choice for expressing the concept of information.  Entropy, a weighted measure of information is basically the expected value of information present. In other words

If there are <eqn>n</eqn> messages <eqn> X= {X_1, ..., X_n}</eqn> with probabilities <eqn>p(X_1) ... p(X_n)</eqn> then the Shannon entropy of this set is <a href="http://www.cs.utsa.edu/~wagner/laws/coding.html">defined as</a>:

<eqn>H(X)= -p(X_1) \log_2 p(X_1) + ... + p(X_n) \log_2 p(X_n)</eqn>


or in other words

<eqn>H(X)= E ( I(X)</eqn>

Entropy is maximum when all values are equiprobable.

Information is defined as

<eqn>I_S(X)=H_{max} - H(X)</eqn>

Information in the Shannon sense is defined as the change in entropy before and after a particular event has taken place. Shannon information, also known as surprise, is any form of data which is not already known. In fact, when rare events occur, they generate a lot of information. 


Tom Schneider has some good resources:


<list><li><a href="http://www.lecb.ncifcrf.gov/~toms/paper/primer/">Information Theory Primer</a>
</li><li><a href="http://www.lecb.ncifcrf.gov/~toms/paper/nano2/">Sequence Logos, Machine/Channel Capacity, Maxwell's Demon, and Molecular Computers: a Review of the Theory of Molecular Machines </a>
</li><li><a href="http://www.lecb.ncifcrf.gov/~toms/paper/trieste1996/">New Approaches in Mathematical Biology: Information Theory and Molecular Machines </a>
</li></list>

So what we have learned so far is that Dembski's information measure is nothing more than a probability measure similar to Shannon's <b>entropy</b> measure not Shannon's <b>information</b> measure.


But the choice of the term information is quite unfortunate since it has more similarity to entropy than to Shannon information. 

So let's try to understand why Dembski argues that regularity and chance cannot create CSI. The answer is simple: If such processes have a high probability of being successful, their <i>Dembski information</i> measure will be low.

But the same problem applies to Intelligent designers. Given a particular 'intelligently designed' event, its probability is high and thus its information is low. In other words, according to Dembski's own measure, nothing can create CSI other than pure chance.

Not much of a useful tool but the poor choice of the information measure has caused much unnecessary confusion. When in fact all Dembski was doing is repeating the age-old creationist argument that evolution or abiogenesis is improbable.

<a href="http://www.talkorigins.org/faqs/abioprob/">Talkorigins</a> has some good FAQ's on what's wrong with these arguments.

It seems that ID is not only fundamentally flawed due to a theoretical failure of its claims but also empirically flawed in that ID has failed to be scientifically relevant. But in addition to these flaws, we also recognize the flawed arguments of Dembski based on probability. All because of the confusing usage of terms like information rather than entropy.
Seems that the intelligent designer is as powerless in creating CSI. Or alternatively an intelligent designer is as capable of creating CSI as regular processes. 


<a href="http://www.lecb.ncifcrf.gov/~toms/paper/ev/dembski/specified.complexity.html">Tom Schneider</a> attracted Dembski's ire for showing how the simple processes of variation and selection can actually <a href="http://www.lecb.ncifcrf.gov/~toms/paper/ev/">increase the information</a> in a genome.

Dembski's <a href="http://home.mira.net/~reynella/debate/dembski.htm">complexity measures</a> have many problems. 

Surprisingly various ID proponents such as for instance <a href="http://www.freerepublic.com/forum/a3a2acd8f19d5.htm">Fred Heeren</a> seem to have taken Dembski's claims too seriously.

Heeren quotes another unsupported and in fact falsified claim by Dembski

<quote>
William Dembski puts it this way: "Specified complexity powerfully extends the usual mathematical theory of information, known as Shannon information. Shannon's theory dealt only with complexity, which can be due to random processes as well as to intelligent design. The addition of specification to complexity, however, is like a vise that grabs only things due to intelligence. Indeed, all the empirical evidence confirms that the only known cause of specified complexity is intelligence."
</quote>

Careless usage of terminology, contradictory statements and examples, confusing usage of terms and inflated claims all seem to have made the design inference 'quite problematic'.

<a href="http://www.metanexus.net/metanexus_online/show_article.asp?4445">Branden Fitelson</a> 

<quote>
Understanding what "regularity," "chance," and "design" mean in Dembski's framework is made more difficult by some of his examples. Dembski discusses a teacher who finds that the essays submitted by two students are nearly identical (46). One hypothesis is that the students produced their work independently; a second hypothesis asserts that there was plagiarism. Dembski treats the hypothesis of independent origination as a Chance hypothesis and the plagiarism hypothesis as an instance of Design. Yet, both describe the matching papers as issuing from intelligent agency, as Dembski points out (47). Dembski says that context influences how a hypothesis gets classified (46). How context induces the classification that Dembski suggests remains a mystery. 
</quote>


Elsberry and Shallit have written an excellent paper <a href="http://www.talkreason.org/articles/eandsdembski.pdf">"Information Theory, Evolutionary Computation, and Dembski's "Complex Specified Information"</a>. They address Dembski's fallacious reliability claims, present the differences between rarefied design and ordinary design, and the problems with apparant and actual complex specified information (CSI).

<quote>
Intelligent design advocate William Dembski has introduced a measure of information called "complex specified information", or CSI. He claims that CSI is a reliable marker of design by intelligent agents. He puts forth a "Law of Conservation of Information" which states that chance and natural laws are incapable of generating CSI.
In particular, CSI cannot be generated by evolutionary computation. Dembski asserts that CSI is present in intelligent causes and in the flagellum of Escherichia coli, and concludes that neither have natural explanations. In this paper we examine Dembski's claims, point out significant errors in his reasoning, and conclude that there is no reason to accept his assertions.
</quote>
